{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marine-egyptian",
   "metadata": {
    "tags": [
     "Intro"
    ]
   },
   "source": [
    "https://www.sbert.net/index.html\n",
    "1. Separar em tópicos (SBERT - assymetric semantic search / clustering / topic modelling)\n",
    "2. Encontrar perguntas dentro de tópicos relevantes (manual)\n",
    "3. Obter sinônimos para aquelas perguntas (SBERT - symmetric semantic search)\n",
    "4. Extrair pares pergunta-resposta (IBM - domain-specific-QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-poverty",
   "metadata": {},
   "source": [
    "# Assymmetric Semantic Search\n",
    "For asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like “What is Python” and you wand to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.   \n",
    "<br>\n",
    "### Suitable models for assymmetric semantic search:\n",
    "\n",
    "- msmarco-distilbert-base-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-quality",
   "metadata": {},
   "source": [
    "# Symmetric Semantic Search\n",
    "For symmetric semantic search your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be “How to learn Python online?” and you want to find an entry like “How to learn Python on the web?”. For symmetric tasks, you could potentially flip the query and the entries in your corpus.   \n",
    "<br>\n",
    "### Suitable models for symmetric semantic search:\n",
    "\n",
    "- paraphrase-distilroberta-base-v1 / paraphrase-xlm-r-multilingual-v1\n",
    "\n",
    "- quora-distilbert-base / quora-distilbert-multilingual\n",
    "\n",
    "- distiluse-base-multilingual-cased-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-vegetation",
   "metadata": {},
   "source": [
    "# Extraindo apenas queries do MSMARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abre o json\n",
    "import json\n",
    "path = r\"data/train_v2.1.json\"\n",
    "with open(path, 'r') as f:\n",
    "    file = f.read()\n",
    "    msmarco = json.loads(file)\n",
    "del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pega as queries e passa para string\n",
    "msmarco_queries = ';'.join(msmarco['query'].values())\n",
    "del msmarco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# escreve a string num arquivo\n",
    "with open(\"data/queries.txt\", \"w\", encoding='utf8') as f2:\n",
    "    f2.write(msmarco_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê a string do arquivo\n",
    "with open(\"data/queries.txt\", \"r\", encoding='utf8') as f3:\n",
    "    msmarco_queries = f3.read()\n",
    "print(msmarco_queries[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-original",
   "metadata": {},
   "source": [
    "# Topic modelling with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9703f",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demorado! ~3 GB\n",
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificar se está instalado corretamente\n",
    "import torch\n",
    "print(torch.cuda.is_available()) # True\n",
    "print(torch.cuda.current_device()) # int\n",
    "print(torch.cuda.device_count()) # >0\n",
    "print(torch.cuda.get_device_name(0)) # GeForce ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd705a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0020dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import umap\n",
    "import os\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pd.set_option(\"display.max_rows\", 10, \"display.max_columns\", None, \"display.width\", None, \"display.max_colwidth\", 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6714e",
   "metadata": {},
   "source": [
    "## Obter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/queries.txt\", \"r\", encoding='utf8') as f3:\n",
    "    msmarco_queries = f3.read()\n",
    "data = msmarco_queries.split(';')\n",
    "del msmarco_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "display(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c7ebb",
   "metadata": {},
   "source": [
    "## Obter embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(data, show_progress_bar=True, device='cuda')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30e0b9",
   "metadata": {},
   "source": [
    "Não vale a pena salvar os embeddings, pois ocupa muito espaço (16 GB), demora mais para salvar que gerar novamente e não consegui carregar de volta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbe99d",
   "metadata": {},
   "source": [
    "## Obter umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduz embeddings de 768 componentes para 5\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-alfred",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# salvar umap_embeddings\n",
    "umap_path = os.path.join(os.getcwd(), 'umap_embeddings.txt')\n",
    "with open(umap_path, 'w') as file:\n",
    "    for row in tqdm(umap_embeddings):\n",
    "        np.savetxt(file, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# carregar umap_embeddings\n",
    "try:\n",
    "    umap_embeddings\n",
    "except:\n",
    "    umap_path = os.path.join(os.getcwd(), 'umap_embeddings.txt')\n",
    "    umap_embeddings = np.loadtxt(umap_path).reshape(809214, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e4e9a",
   "metadata": {},
   "source": [
    "## Obter clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217d1ce",
   "metadata": {},
   "source": [
    "## Visualizar\n",
    "Opcional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Visualização\n",
    "# Faz uma nova reducao dos embeddings para apenas 2 componentes (extremamente demorado!)\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3f4ee",
   "metadata": {},
   "source": [
    "## Analisar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados em dataframe\n",
    "# Doc = query, Topic = cluster\n",
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=10)\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# palavras para procurar dentre os clusters\n",
    "to_find = ['marine', 'ocean', 'sea', 'oil', 'beach', 'current', 'tide', 'wave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b60c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "found = {word : [] for word in to_find}\n",
    "for i in range(len(top_n_words) - 1):\n",
    "    for word in to_find:\n",
    "        if word in [a for a,b in top_n_words[i]]:\n",
    "            found[word].append(i)\n",
    "            \n",
    "display(found)  # clusters que possuem as palavras procuradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecf2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_find = ['marine', 'ocean', 'sea', 'oil', 'beach', 'current', 'tide', 'wave']\n",
    "for i in found['wave']:\n",
    "    display(i, top_n_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = docs_df[docs_df['Topic'] == 3908]\n",
    "display(s)\n",
    "print(s.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40293ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultado\n",
    "inter = [872, 905, 1885, 891, 1616, 1651, 1652, 3779, 1421, 1913, 885]  # clusters possivelmente contendo perguntas interessantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df = docs_df[docs_df['Topic'].apply(lambda x: x in inter)]  # linhas pertencentes aos clusters interessantes\n",
    "display(inter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_json = inter_df.drop(columns=['Topic', 'Doc_ID'])\n",
    "inter_json.to_json('pre_lookup_table.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16f599",
   "metadata": {},
   "source": [
    "## Reduzir a quantidade de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC REDUCTION\n",
    "for i in range(20):\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tf_idf.T)\n",
    "    np.fill_diagonal(similarities, 0)\n",
    "\n",
    "    # Extract label to merge into and from where\n",
    "    topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
    "    topic_to_merge = topic_sizes.iloc[-1].Topic\n",
    "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "    # Adjust topics\n",
    "    docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
    "    old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
    "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
    "    docs_df.Topic = docs_df.Topic.map(map_topics)\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "    # Calculate new topic words\n",
    "    m = len(data)\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28edc",
   "metadata": {},
   "source": [
    "# Domain-Specific QA (IBM)\n",
    "Inspirado em: https://github.com/ibm-aur-nlp/domain-specific-QA  \n",
    "O código da IBM *não filtra* os tópicos, apenas pega a lista de ids já filtrados e gera  \n",
    "o json contendo todas as informações relevantes a cada pergunta em cada domínio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 426.388,
   "position": {
    "height": "448.388px",
    "left": "979px",
    "right": "20px",
    "top": "96px",
    "width": "530px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
